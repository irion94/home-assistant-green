"""Pydantic models for AI Gateway.

This module defines all request/response schemas and data models used
throughout the application.
"""

from __future__ import annotations

from typing import Any, Literal

from pydantic import BaseModel, Field
from pydantic_settings import BaseSettings, SettingsConfigDict


class Config(BaseSettings):
    """Application configuration from environment variables."""

    model_config = SettingsConfigDict(env_file=".env", case_sensitive=False)

    ha_token: str = Field(..., description="Home Assistant long-lived access token")
    ha_base_url: str = Field(
        default="http://homeassistant:8123", description="Home Assistant base URL"
    )
    ollama_base_url: str = Field(
        default="http://host.docker.internal:11434", description="Ollama API base URL"
    )
    ollama_model: str = Field(default="llama3.2:3b", description="Ollama model to use")
    log_level: str = Field(default="INFO", description="Logging level")


class AskRequest(BaseModel):
    """Request schema for /ask endpoint."""

    text: str = Field(..., description="Natural language command", min_length=1)


class HAAction(BaseModel):
    """Home Assistant action plan generated by Ollama."""

    action: Literal["call_service", "none"] = Field(
        ..., description="Type of action to perform"
    )
    service: str | None = Field(
        None, description="HA service to call (e.g., 'light.turn_on')"
    )
    entity_id: str | None = Field(None, description="Target entity ID")
    data: dict[str, Any] | None = Field(default_factory=dict, description="Service data")


class AskResponse(BaseModel):
    """Response schema for /ask endpoint."""

    status: Literal["success", "error"] = Field(..., description="Request status")
    plan: HAAction | None = Field(None, description="Generated action plan")
    message: str | None = Field(None, description="Error or status message")
    ha_response: dict[str, Any] | list[Any] | None = Field(
        None, description="Response from Home Assistant API"
    )


class OllamaRequest(BaseModel):
    """Request schema for Ollama API."""

    model: str = Field(..., description="Model name")
    messages: list[dict[str, str]] = Field(..., description="Chat messages")
    stream: bool = Field(default=False, description="Enable streaming")
    format: Literal["json"] = Field(default="json", description="Force JSON output")


class OllamaResponse(BaseModel):
    """Response schema from Ollama API."""

    model: str
    message: dict[str, str]
    done: bool
    total_duration: int | None = None
    load_duration: int | None = None
    prompt_eval_duration: int | None = None
    eval_duration: int | None = None
